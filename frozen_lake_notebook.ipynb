{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvironmentModel:\n",
    "    \n",
    "    def __init__(self, n_states, n_actions, seed=None):\n",
    "        self.n_states = n_states\n",
    "        self.n_action = n_actions\n",
    "\n",
    "        self.random_state = np.random.RandomState(seed)\n",
    "\n",
    "    def p(self, next_state, state, action):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def r(self, next_state, state, action):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def draw(self, state, action):\n",
    "        p = [self.p(ns, state, action) for ns in range(self.n_states)]\n",
    "\n",
    "        next_state = self.random_state.choice(self.n_states, p=p)\n",
    "        reward = self.r(next_state, state, action)\n",
    "\n",
    "        return next_state, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(EnvironmentModel):\n",
    "    def __init__(self, n_states, n_actions, max_steps, dist, seed = None):\n",
    "        EnvironmentModel.__init__(self, n_states, n_actions, seed)\n",
    "        self.n_actions = n_actions\n",
    "        self.max_steps = max_steps\n",
    "        \n",
    "        self.dist = dist\n",
    "        if self.dist is None:\n",
    "            self.dist = np.full(n_states, 1./n_states) #returns even distribution\n",
    "            \n",
    "    def reset(self):\n",
    "        self.n_steps = 0\n",
    "        self.state = self.random_state.choice(self.n_states, p = self.dist)\n",
    "        \n",
    "        return self.state\n",
    "    \n",
    "    \n",
    "    def step(self, action):\n",
    "        if action < 0 or action >= self.n_actions:\n",
    "            raise Exception('Invalid_action.')\n",
    "            \n",
    "        self.n_steps += 1\n",
    "        done = (self.n_steps >= self.max_steps)\n",
    "        \n",
    "        self.state, reward = self.draw(self.state, action)\n",
    "        \n",
    "        return self.state, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gridWorld(Environment):\n",
    "    \n",
    "    def __init__(self, size,lakes,goals, n_actions = 4, max_steps = 100, dist = None, seed = None, rnd=0.1):\n",
    "        n_states = (size[0]*size[1])+1\n",
    "        Environment.__init__(self, n_states, n_actions, max_steps,dist)\n",
    "        self.create_dicts_and_indexes(size,lakes,goals)\n",
    "        self.create_board(size, lakes,goals)\n",
    "        self.action_dict = {\"Up\":0, \"Right\":1, \"Down\":2,\"Left\":3}\n",
    "        self.chance = rnd\n",
    "        self._init_probs_dict()\n",
    "        self.reset()\n",
    "        self.rnd = rnd\n",
    "        \n",
    "    def p(self,next_state, state, action):\n",
    "        \"\"\"\n",
    "        Here, based on a 'chosen' action, we give the probability of transitioning from one state to another\n",
    "        Functions:\n",
    "          We calculate the probability if the chosen action is the 'actual' action, multiplied by chance of not taking random action\n",
    "          We then add the probability for each action, multiplied by chance of taking random action / number of actions\n",
    "        \"\"\"\n",
    "        no_rnd = 1 - self.rnd\n",
    "        probas = 0\n",
    "        probas += no_rnd * self.SAS_probs[state][action][next_state]\n",
    "        for a in range(self.n_actions):\n",
    "            probas += (self.rnd/self.n_actions) * self.SAS_probs[state][a][next_state]\n",
    "        return probas\n",
    "        \"The method p returns the probability of transitioning from state to next state given action. \"\n",
    "        \n",
    "    def r(self, next_state, state, action):\n",
    "        \"The method r returns the expected reward in having transitioned from state to next state given action.\"\n",
    "        return self.goal_states_idx[state] if state in self.goal_states_idx else 0\n",
    "    \n",
    "    def render(self):\n",
    "        board = self.board.copy()\n",
    "        posR, posC = self.stateIdx_to_coors[self.state]\n",
    "        board[posR, posC] = \"P\"\n",
    "        clear_output()\n",
    "        print(board)\n",
    "        \n",
    "    def create_dicts_and_indexes(self,size, lakes, goal_states, terminal_state = True):\n",
    "        \"\"\"\n",
    "        Inputs... \n",
    "         size of lake (tuple e.g. (4,4))\n",
    "         Location of lakes in coordinate form e.g. [(0,1),(1,2)...]\n",
    "         Location of goal_states and their rewards e.g. {(3:3):1, (5,5):-1} In our examples this is always just one goal state\n",
    "\n",
    "        Outputs...\n",
    "         Dictionary linking coordinates to index of each state, and reverse dictionary\n",
    "         Lake squares in index form e.g. [3,6,9]\n",
    "         Goal states in index form e.g {15: 1, 25: -1}\n",
    "        \"\"\"\n",
    "        self.lakes = lakes\n",
    "        self.goal_states = goal_states\n",
    "        self.h = size[0]\n",
    "        self.w = size[1]\n",
    "        self.coors_to_stateIdx = {}\n",
    "        idx =0\n",
    "        for r in range(self.h):\n",
    "            for c in range(self.w):\n",
    "                self.coors_to_stateIdx[(r,c)] = idx\n",
    "\n",
    "                idx+=1\n",
    "\n",
    "        if terminal_state:\n",
    "            self.coors_to_stateIdx[(-1,-1)] = self.n_states-1\n",
    "            self.terminal_state = self.n_states-1\n",
    "\n",
    "        self.stateIdx_to_coors = {}\n",
    "        for k,v in self.coors_to_stateIdx.items():\n",
    "            self.stateIdx_to_coors[v]=k\n",
    "        self.lakes_idx = [self.coors_to_stateIdx[x] for x in lakes]\n",
    "        self.goal_states_idx = {self.coors_to_stateIdx[k]:v for k,v in goal_states.items()}\n",
    "\n",
    "\n",
    "    def create_board(self,size, lakes, goal_states):\n",
    "        \"\"\"\n",
    "        Inputs: size of lake (h and w), coordinate location of lakes, and coordinate location and value of goal states\n",
    "        Outputs: array of player-less board, with lake locations and reward locations\n",
    "        \"\"\"\n",
    "        ### Creation of board object\n",
    "        h,w = size[0],size[1]\n",
    "        self.board = np.array(['_'] * h*w).reshape(h,w)\n",
    "        for l in lakes:\n",
    "            self.board[l] = 'L'\n",
    "        for g, r in goal_states.items():\n",
    "            self.board[g] = r\n",
    "    \n",
    "    def _init_probs_dict(self):\n",
    "        \"\"\"\n",
    "        In: the backend of the board (stateIdx_to_coors dict, lakes, goals, terminal state)\n",
    "        Out: returns the impact of an ACTUAL action on the board position of a player\n",
    "        Structure of output: {Current_State1: {Up: state1, state2, state 3....,\n",
    "                            Down: state1, state2, state 3...}\n",
    "                            ....\n",
    "                    Current_State2: {Up ......}}\n",
    "        \n",
    "        note: 'actual' action distinguished here from 'chosen' action. Players 'choose', then we apply randomness, and then there is an 'actual' action\n",
    "        This function concerns the effect of an 'actual' action on the position of a player.\n",
    "        \"\"\"\n",
    "        \n",
    "        ### HELPER FUNCTIONS\n",
    "        def state_is_top(state):\n",
    "            return stateIdx_to_coors[state][0] == 0\n",
    "        def state_is_bottom(state):\n",
    "            return stateIdx_to_coors[state][0] == self.h-1\n",
    "        def state_is_left(state):\n",
    "            return stateIdx_to_coors[state][1] == 0\n",
    "        def state_is_right(state):\n",
    "            return stateIdx_to_coors[state][1] == self.w-1\n",
    "        def move_up():\n",
    "            return -self.w\n",
    "        def move_down():\n",
    "            return self.w\n",
    "        def move_left():\n",
    "            return -1\n",
    "        def move_right():\n",
    "            return 1\n",
    "        \n",
    "        SA_prob_dict = {}\n",
    "        lakes_and_goals = list(self.goal_states_idx.keys()) + self.lakes_idx\n",
    "        \n",
    "        for state in range(self.n_states-1):\n",
    "            SA_prob_dict[state] = {}\n",
    "            #### Set the chance of entering an absorbing from lake or goal to 1\n",
    "            for i in range(4):\n",
    "                SA_prob_dict[state][i] = np.zeros((self.n_states,))\n",
    "                if state in lakes_and_goals:\n",
    "                    for act in range(4):\n",
    "                        SA_prob_dict[state][i][self.n_states-1] = 1\n",
    "            \n",
    "            if state not in lakes_and_goals:\n",
    "                \"\"\"For UP\"\"\"\n",
    "                if not state_is_top(state): #if you're in a normal state, you'll just go up 1\n",
    "                    SA_prob_dict[state][self.action_dict['Up']][state+move_up()] = 1\n",
    "                else:\n",
    "                    SA_prob_dict[state][self.action_dict['Up']][state] = 1\n",
    "\n",
    "                \"\"\"For DOWN\"\"\"\n",
    "                if not state_is_bottom(state): #if you're in a normal state, you'll just go up 1\n",
    "                    SA_prob_dict[state][self.action_dict['Down']][state+move_down()] = 1\n",
    "                else:\n",
    "                    SA_prob_dict[state][self.action_dict['Down']][state] = 1\n",
    "\n",
    "                \"\"\"For LEFT\"\"\"\n",
    "                if not state_is_left(state): #if you're in a normal state, you'll just go up 1\n",
    "                    SA_prob_dict[state][self.action_dict['Left']][state+move_left()] = 1\n",
    "                else:\n",
    "                    SA_prob_dict[state][self.action_dict['Left']][state] = 1\n",
    "\n",
    "                \"\"\"For RIGHT\"\"\"\n",
    "                if not state_is_right(state): #if you're in a normal state, you'll just go up 1\n",
    "                    SA_prob_dict[state][self.action_dict['Right']][state+move_right()] = 1\n",
    "                else:\n",
    "                    SA_prob_dict[state][self.action_dict['Right']][state] = 1     \n",
    "        self.SAS_probs = SA_prob_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (4,4)\n",
    "lakes = [(1,1),(1,3),(2,3),(3,0)]\n",
    "goals = {(3,3):1}\n",
    "dist = np.zeros((size[0]*size[1]+1))\n",
    "dist[0]=1\n",
    "env=gridWorld(size,lakes,goals, n_actions = 4, max_steps = 100, dist = dist, seed = None, rnd=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['_' '_' '_' '_']\n",
      " ['_' 'L' '_' 'P']\n",
      " ['_' '_' '_' 'L']\n",
      " ['L' '_' '_' '1']]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Move:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game Over, Reward: 0\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "env.render()\n",
    "actions = [0,1,2,3]\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    c = int(input('\\nMove: '))\n",
    "    if c not in actions:\n",
    "        raise Exception('Invalid_action')\n",
    "        \n",
    "    state, r, done = env.step(actions.index(c))\n",
    "    if state == env.terminal_state: \n",
    "        done == True; break\n",
    "        \n",
    "    env.render()\n",
    "print(f\"Game Over, Reward: {r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
